# Generative-Artificial-Intelligence-Document-QA-based-on-RAG

This repository contains my implementation and report for **HW3 (Document QA based on Retrieval-Augmented Generation)** in the course *Generative Artificial Intelligence*.

The goal is to build a lightweight RAG pipeline that answers questions about NLP papers by retrieving relevant evidence from each paper’s `full_text`, then generating a concise answer grounded in the retrieved chunks.

---

## Contents

- `112101014.py` — Main runnable RAG script (public/private mode).
- `rag-v2.ipynb` — Course tutorial / experimental notebook.
- `public_dataset.json` — 100-paper public set with gold **answer** and **evidence** (grading set).
- `private_dataset.json` — Larger unlabeled set for development and semi-supervised ideas.
- `sample_submission.json` — Output format example.
- `112101014.json` — Example output generated by my system.
- `112101014.pdf` — Written report.
- `[GAI25] RAG Lecture & HW3.pptx` — Lecture slides.
- `HW3 RAG (Report 評分標準v1).pdf` — Report rubric.

---

## Task Summary

For each paper entry:

1. Split `full_text` into chunks.
2. Build a vector index over chunks.
3. Retrieve top-*k* chunks for the given `question`.
4. Generate:
   - `answer`: short, direct, ideally extractive.
   - `evidence`: the retrieved text snippets.

Evaluation emphasizes **faithfulness** and **conciseness** using **ROUGE-L** on both the answer and evidence.

---

## Method (high-level)

The implementation in `112101014.py` provides a clean baseline RAG workflow:

- **Chunking** with `RecursiveCharacterTextSplitter`  
  - `CHUNK_SIZE = 256`  
  - `CHUNK_OVERLAP = 64`
- **Vector store** using **FAISS**.
- **Retriever**: top-*k* dense retrieval (`RETRIEVE_TOP_K = 5`).
- **Generator**:
  - Default: Groq-hosted `llama-3.1-8b-instant`
  - Alternative: local VLLM endpoint (toggle `USING_GROQ = False`).
- **Embeddings**:
  - Prefer **Ollama** (`snowflake-arctic-embed2:568m-l-fp16`) if available.
  - Fallback: **HuggingFace** `BAAI/bge-small-en-v1.5`.

My report further explores stronger configurations (hybrid retrieval, re-ranking, prompt constraints, and semi-supervised ideas).

---

## Environment

Recommended:

- Python **3.10+**
- A CUDA-capable GPU is helpful but not strictly required.

Key packages (aligned with the course notebook):

```bash
pip install \
  pandas==2.2.3 \
  langchain==0.3.23 \
  langchain-community==0.3.21 \
  langchain-groq==0.3.2 \
  langchain-ollama==0.3.1 \
  rich==14.0.0 \
  openai==1.71.0 \
  numpy<2 \
  rouge-score \
  tqdm \
  faiss-cpu
```

> If you want GPU FAISS, replace `faiss-cpu` with a compatible `faiss-gpu` build for your environment.

---

## Setup

1. Place datasets in the project root:
   - `public_dataset.json`
   - `private_dataset.json`

2. **Set API keys securely** (recommended):

```bash
export GROQ_API_KEY="YOUR_KEY"
```

3. Open `112101014.py` and ensure:

- `USING_GROQ = True`
- Replace hard-coded keys with environment-based loading.

---

## Usage

### Run on public dataset

```bash
python 112101014.py --output 112101014_public.json
```

### Run on private dataset

```bash
python 112101014.py --private --output 112101014_private.json
```

Outputs follow the required schema:

```json
[
  {
    "title": "...",
    "answer": "...",
    "evidence": ["...", "..."]
  }
]
```

---

## Reproducibility Tips

- Fix random seeds if you add training or re-ranking stages.
- Log:
  - chunking parameters
  - embedding model name & version
  - retrieval *k*
  - prompt template
  - LLM name, temperature, max tokens

---

## Notes on Academic Integrity & Safety

- Avoid committing private API keys to version control.
- Ensure answers are strictly grounded in retrieved context.
- Prefer short, extractive answers to maximize ROUGE-L and reduce hallucinations.

---

## References & Resources

- Course lecture slides: `[GAI25] RAG Lecture & HW3.pptx`
- HW3 tutorial notebook: `rag-v2.ipynb`
- Lecture video (YouTube): https://youtu.be/TYhC2tpJcxY

---

## License

This project is for coursework and academic demonstration.
